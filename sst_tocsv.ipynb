{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sst_tocsv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "history_visible": true,
      "mount_file_id": "1f-KsRHXZBSUe4y0dR1IiyUV3B9f87nTJ",
      "authorship_tag": "ABX9TyMFiWLgGjebQ0qXv7iEjhd5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amaslov455/nlp_project/blob/main/sst_tocsv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0ep8yEAJh0T",
        "outputId": "de31f29f-db9d-45f3-c604-3de93e4b8809"
      },
      "source": [
        "!pip install pytreebank"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytreebank\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/12/626ead6f6c0a0a9617396796b965961e9dfa5e78b36c17a81ea4c43554b1/pytreebank-0.2.7.tar.gz\n",
            "Building wheels for collected packages: pytreebank\n",
            "  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytreebank: filename=pytreebank-0.2.7-cp37-none-any.whl size=37070 sha256=8881648f3d8eadba675f91d524ce07f52e5699ab92c94b0ebc74a8092d75db36\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/b6/91/e9edcdbf464f623628d5c3aa9de28888c726e270b9a29f2368\n",
            "Successfully built pytreebank\n",
            "Installing collected packages: pytreebank\n",
            "Successfully installed pytreebank-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELj0aVhZJ8Rr"
      },
      "source": [
        "import pytreebank\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so9ZaSiXKDJP"
      },
      "source": [
        "dataset = pytreebank.load_sst()\n",
        "fine_grained = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLIpPh_tRPib",
        "outputId": "ce81f3d4-cb13-40d6-dca3-51b99e9d027d"
      },
      "source": [
        "dataset.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train', 'test', 'dev'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgr-LiYtbYvW"
      },
      "source": [
        "def create_df_from_treebank(input_dataset):\n",
        "  dict_ = {}\n",
        "  dict_['sentence'] = []\n",
        "  dict_['santiment'] = []\n",
        "\n",
        "  list_santiments = [\"very_negative\", \"negative\", \"neutral\", \"positive\", \"very_positive\"]\n",
        "\n",
        "  for part in input_dataset:\n",
        "    label, sentence = part.to_labeled_lines()[0]\n",
        "\n",
        "    dict_['sentence'].append(sentence)\n",
        "    dict_['santiment'].append(list_santiments[label])\n",
        "\n",
        "  df = pd.DataFrame.from_dict(dict_)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wXdtiH1Rqoa"
      },
      "source": [
        "df_train = create_df_from_treebank(dataset['train'])\n",
        "df_test = create_df_from_treebank(dataset['test'])\n",
        "df_valid = create_df_from_treebank(dataset['dev'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTxBWV_lSO9y"
      },
      "source": [
        "# Create SST-2 from SST-5\n",
        "# Use only if needed\n",
        "\n",
        "def no_fine_grainedSST(df):\n",
        "  df_return = df.copy()\n",
        "  df_return = df_return[df_return['santiment'] != 'neutral']\n",
        "  df_return = df_return.replace({'santiment': {'very_positive': 'positive', 'very_negative': 'negative'}})\n",
        "  return df_return\n",
        "\n",
        "if fine_grained:\n",
        "  df_train = no_fine_grainedSST(df_train)\n",
        "  df_test = no_fine_grainedSST(df_test)\n",
        "  df_valid = no_fine_grainedSST(df_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjuRVIgnwmW8",
        "outputId": "32104ad0-c66e-4563-8c4d-361483cd767c"
      },
      "source": [
        "print(set(df_train['santiment']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'very_positive', 'very_negative', 'positive', 'neutral', 'negative'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3o_Qfy7M2ky"
      },
      "source": [
        "df_train.to_csv('/content/drive/MyDrive/diplom_project/train.csv', index = False)\n",
        "df_test.to_csv('/content/drive/MyDrive/diplom_project/test.csv', index = False)\n",
        "df_valid.to_csv('/content/drive/MyDrive/diplom_project/valid.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaFOkgHR34EQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d125c101-ec1f-48bb-a806-1fc5f82348cc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "joined_sen = ' '.join(df_train['sentence'])\n",
        "\n",
        "tokens = nltk.word_tokenize(joined_sen)\n",
        "print('count of all tokens: ', len(tokens))\n",
        "\n",
        "unique_tokens = list(set(tokens))\n",
        "print('count of unique tokens: ', len(unique_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "count of all tokens:  163642\n",
            "count of unique tokens:  18270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMlSWux_SQgU"
      },
      "source": [
        "def write_sents_to_txt(list_of_sents, filename):\n",
        "    with open(filename, 'w',encoding='utf-8') as f:\n",
        "        for text in list_of_sents:\n",
        "            f.write(text + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaTWvzuZecPH"
      },
      "source": [
        "# Export df_train['sentence'] to txt\n",
        "DIR_TXT_FILE = '/content/drive/MyDrive/diplom_project/sents_to_trainSPM.txt'\n",
        "\n",
        "write_sents_to_txt(list(df_train['sentence'].values), DIR_TXT_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyoZCuxRf8nS"
      },
      "source": [
        "# Test sentencepiece does it work fine\n",
        "# Use only to test library\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "\n",
        "vocab_size_totrain = 10000\n",
        "spm.SentencePieceTrainer.train('--input={} --model_prefix=m --vocab_size={}'.format(DIR_TXT_FILE, vocab_size_totrain))\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "df_train['joined_nltk'] = df_train['sentence'].apply(lambda x: ' '.join(nltk.word_tokenize(x)))\n",
        "df_train['joined_sentencepiece'] = df_train['sentence'].apply(lambda x: ' '.join(sp.encode_as_pieces(x)))\n",
        "\n",
        "print(df_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7gpMWQzIMV1",
        "outputId": "da444591-1970-4a2d-df4a-4d434bb92e99"
      },
      "source": [
        "len(unique_tokens)//500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ8_Tn_Owypc",
        "outputId": "2c9e3ccd-8a72-4067-ca1b-e52e16ef8804"
      },
      "source": [
        "len(unique_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18270"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "vuGIbMVaIQfu",
        "outputId": "7c5237f0-6b29-44e8-913f-a5dec1d07e29"
      },
      "source": [
        "# Divide all datasets to custom number of tokens\n",
        "\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "from tqdm import tqdm\n",
        "\n",
        "step_size = 500\n",
        "number_of_parts = len(unique_tokens)//step_size\n",
        "\n",
        "for i in tqdm(range(number_of_parts)):\n",
        "  vocab_size_totrain = (i+1) * step_size\n",
        "\n",
        "  # spm.SentencePieceTrainer.train('--input={} --model_prefix=m --vocab_size={}'.\\\n",
        "  #                                format(DIR_TXT_FILE, vocab_size_totrain))\n",
        "  spm.SentencePieceTrainer.train(input=DIR_TXT_FILE,\n",
        "                                 model_prefix='m',\n",
        "                                 vocab_size=vocab_size_totrain)\n",
        "  sp = spm.SentencePieceProcessor()\n",
        "  sp.load('m.model')\n",
        "\n",
        "  df_train['joined_spm_{}'.format(vocab_size_totrain)] = df_train['sentence'].apply(lambda x: ' '.join(sp.encode_as_pieces(x)))\n",
        "  df_test['joined_spm_{}'.format(vocab_size_totrain)] = df_test['sentence'].apply(lambda x: ' '.join(sp.encode_as_pieces(x)))\n",
        "  df_valid['joined_spm_{}'.format(vocab_size_totrain)] = df_valid['sentence'].apply(lambda x: ' '.join(sp.encode_as_pieces(x)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Using cached https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 61%|██████    | 22/36 [00:43<00:18,  1.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9281445f07a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   spm.SentencePieceTrainer.train(input=DIR_TXT_FILE,\n\u001b[1;32m     16\u001b[0m                                  \u001b[0mmodel_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                  vocab_size=vocab_size_totrain)\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mSentencePieceTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36m_TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceTrainer__TrainFromMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Internal: /sentencepiece/python/bundled/sentencepiece/src/trainer_interface.cc(579) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (11500). Please set it to a value <= 11107."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQQIQeu8M0Hf",
        "outputId": "6ec934b0-9f35-4033-82c1-02aa72555a54"
      },
      "source": [
        "len(set(' '.join(list(df_test['joined_spm_1000'])).split()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1018"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7totkd7d3LVp"
      },
      "source": [
        "# Export train/test/validation datasets to csv\n",
        "\n",
        "df_train.to_csv('/content/drive/MyDrive/diplom_project/SST5_SPM_train.csv', index = False)\n",
        "df_test.to_csv('/content/drive/MyDrive/diplom_project/SST5_SPM_test.csv', index = False)\n",
        "df_valid.to_csv('/content/drive/MyDrive/diplom_project/SST5_SPM_valid.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}